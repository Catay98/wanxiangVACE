<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tongyi Wanxiang VACE (Wan2.1) Open Source - Groundbreaking Video Editing AI</title>
    <meta name="description" content="Explore the groundbreaking open-source VACE model from Alibaba Cloud's Tongyi Wanxiang. Discover how this unified AI model revolutionizes video editing with text-to-video, image-referenced generation, local editing, video extension, and more.">
    <meta name="keywords" content="Tongyi Wanxiang, VACE, Wan2.1, Open Source, AI Video Editing, Video Generation, Multimodal Model, Alibaba Cloud, AI Video Creation, Generative AI">
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap" rel="stylesheet">

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-F4FKDDJCCJ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-F4FKDDJCCJ');
    </script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            scroll-behavior: smooth;
            background-color: #f7fafc; /* Lighter gray background */
        }
        body[lang="zh-CN"] { /* Apply Noto Sans SC for Chinese */
            font-family: 'Noto Sans SC', sans-serif;
        }
        .hero-bg {
            background: linear-gradient(135deg, #8B5CF6 0%, #C4B5FD 70%, #EDE9FE 100%);
        }
        .section-title {
            border-left: 4px solid #8B5CF6; /* Purple accent */
            padding-left: 0.75rem;
            margin-bottom: 1.5rem;
            font-size: 1.75rem;
            font-weight: 700;
            color: #5B21B6; /* Darker Purple */
        }
        .card {
            background-color: white;
            border-radius: 0.75rem; /* Slightly more rounded */
            padding: 1.5rem;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.07), 0 4px 6px -2px rgba(0, 0, 0, 0.05); /* Softer shadow */
            margin-bottom: 1.5rem;
            border: 1px solid #E5E7EB; /* Light gray border for cards */
        }
        .card h3 {
            font-size: 1.25rem;
            font-weight: 600;
            margin-bottom: 0.75rem;
            color: #6D28D9; /* Purple for card titles */
        }
        .highlight {
            color: #8B5CF6; /* Purple for highlighted text */
            font-weight: 600;
        }
        .feature-list li {
            position: relative;
            padding-left: 1.75rem;
            margin-bottom: 0.5rem;
        }
        .feature-list li::before {
            content: 'âœ“';
            position: absolute;
            left: 0;
            color: #A78BFA; /* Lighter Purple checkmark */
            font-weight: bold;
        }
        .link-button {
            display: inline-block;
            background-color: #8B5CF6; /* Purple button */
            color: white;
            padding: 0.75rem 1.5rem;
            border-radius: 0.375rem;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.3s ease;
        }
        .link-button:hover {
            background-color: #7C3AED; /* Darker Purple on hover */
        }
        .link-button-secondary {
            background-color: #A78BFA; /* Lighter Purple for secondary */
        }
        .link-button-secondary:hover {
            background-color: #8B5CF6;
        }
        .model-version-tag {
            display: inline-block;
            background-color: #EDE9FE; /* Lightest Purple */
            color: #6D28D9;    /* Dark Purple text */
            padding: 0.25rem 0.75rem;
            border-radius: 9999px; /* pill shape */
            font-size: 0.875rem;
            font-weight: 500;
            margin-right: 0.5rem;
            margin-bottom: 0.5rem;
        }
        .nav-link.active {
            font-weight: 700;
            color: #7C3AED; /* Purple for active nav link */
        }
        .footer-link {
            color: #D1D5DB; /* Lighter gray for footer links */
            text-decoration: none;
        }
        .footer-link:hover {
            color: #ffffff;
            text-decoration: underline;
        }
        .language-toggle-container {
            position: fixed;
            top: 1rem;
            right: 1rem;
            z-index: 1001;
        }
        .language-toggle-btn {
            background-color: #8B5CF6;
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            cursor: pointer;
            border: none;
            font-size: 0.875rem;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
        }
        .language-toggle-btn:hover {
            background-color: #7C3AED;
        }
        .main-header {
             background-color: rgba(255, 255, 255, 0.9);
             backdrop-filter: blur(10px);
        }
        @media (max-width: 768px) {
            .main-nav ul {
                background-color: white;
                padding: 1rem;
                border-radius: 0.5rem;
                box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            }
            .language-toggle-container {
                top: 0.75rem;
                right: 0.75rem;
            }
             .language-toggle-btn {
                padding: 0.4rem 0.8rem;
                font-size: 0.8rem;
            }
        }
    </style>
</head>
<body class="text-gray-800 leading-relaxed">

    <div class="language-toggle-container">
        <button id="languageToggleBtn" class="language-toggle-btn">ä¸­æ–‡</button>
    </div>

    <header class="main-header shadow-md sticky top-0 z-50">
        <div class="container mx-auto px-6 py-4 flex justify-between items-center">
            <div>
                <h1 class="text-3xl font-bold text-purple-600">
                    <span data-lang-key="headerVaceTitle">Tongyi Wanxiang</span> <span class="text-pink-500">VACE</span>
                </h1>
                <p class="text-sm text-gray-600" data-lang-key="headerVaceSubtitle">Announcing the Open Source Video Editing Powerhouse</p>
            </div>
            <button id="mobileNavToggle" class="md:hidden mobile-nav-toggle p-2 rounded-md text-gray-600 hover:text-purple-600 hover:bg-purple-100">
                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-6 h-6">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
                </svg>
            </button>
            <nav class="main-nav hidden md:block">
                <ul class="flex space-x-4 md:space-x-6">
                    <li><a href="#latest-updates" class="nav-link text-gray-600 hover:text-purple-600 transition duration-300" data-lang-key="navLatestUpdates">Latest Updates</a></li>
                    <li><a href="#core-features-vace" class="nav-link text-gray-600 hover:text-purple-600 transition duration-300" data-lang-key="navCoreFeatures">Core Features</a></li>
                    <li><a href="#combined-capabilities" class="nav-link text-gray-600 hover:text-purple-600 transition duration-300" data-lang-key="navCombinedCapabilities">Combined Capabilities</a></li>
                    <li><a href="#tech-deep-dive" class="nav-link text-gray-600 hover:text-purple-600 transition duration-300" data-lang-key="navTechDeepDive">Tech Deep Dive</a></li>
                    <li><a href="#getting-started" class="nav-link text-gray-600 hover:text-purple-600 transition duration-300" data-lang-key="navGettingStarted">Get Started</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <section class="hero-bg text-white py-16 md:py-24 px-6" id="hero-section-for-nav">
        <div class="container mx-auto text-center">
            <h2 class="text-4xl md:text-5xl font-bold mb-4" data-lang-key="heroTitle">VACE (Wan2.1) by Tongyi Wanxiang: Open Source Now!</h2>
            <p class="text-xl md:text-2xl mb-8" data-lang-key="heroSubtitle">One model to master diverse video editing tasks, revolutionizing creative workflows.</p>
            <div class="space-x-4">
                <a href="#getting-started" class="link-button text-lg" data-lang-key="heroCTA">Explore VACE &raquo;</a>
            </div>
        </div>
    </section>

    <main class="container mx-auto px-6 py-8 md:py-12">

        <section id="latest-updates" class="mb-12 pt-16 -mt-16">
            <h2 class="section-title" data-lang-key="sectionTitleLatestUpdates">Latest Updates on VACE Open Source</h2>

            <article class="card bg-purple-50 border-purple-200">
                <h3 data-lang-key="article1Title">Tongyi Wanxiang VACE (Wan2.1) Open Sourced: A Unified Model for Diverse Video Editing</h3>
                <p><strong data-lang-key="article1Date">[May 15, 2025]</strong> <span data-lang-key="article1Intro">Alibaba Cloud's Tongyi Wanxiang team has officially open-sourced its groundbreaking VACE (Video Anymator and Composer Engine) model, a core component of the Wan2.1 series. VACE aims to provide a one-stop, efficient, and flexible video creation and editing experience by consolidating multiple complex tasks into a single, powerful model. This release includes VACE-1.3B supporting 480P resolution and VACE-14B supporting both 480P and 720P.</span></p>
                <div class="meta"><span data-lang-key="article1MetaCategory">Category: Latest Updates, Model Release</span> | <a href="#tech-deep-dive" class="text-purple-500 hover:underline" data-lang-key="article1ReadMore">Learn More Technical Details &raquo;</a></div>
                <div class="mt-2">
                    <p><strong data-lang-key="timelineTitle">Key Milestones:</strong></p>
                    <ul class="list-disc list-inside ml-4 text-sm text-gray-700">
                        <li data-lang-key="timelineItem1"><strong>Wan2.1 Series Release:</strong> VACE unveiled as a core component, showcasing its unified video processing capabilities.</li>
                        <li data-lang-key="timelineItem2"><strong>VACE 1.3B & 14B Models Open Sourced:</strong> Powerful models supporting different resolutions (480P/720P) released to the community.</li>
                        <li data-lang-key="timelineItem3"><strong>Active Community Feedback & Exploration:</strong> Developers begin secondary creation, evaluation, and exploring VACE's application potential across various industries.</li>
                    </ul>
                </div>
            </article>

            <article class="card">
                <h3 data-lang-key="article2Title">One Model, Multiple Tasks: How VACE Simplifies Complex Video Editing</h3>
                <p><strong data-lang-key="article2Date">[May 15, 2025]</strong> <span data-lang-key="article2Content">With VACE, users can now seamlessly perform text-to-video generation, image-referenced generation, local video editing, and video duration/spatial extension tasks without frequently switching between different models or tools. This unified approach is set to significantly boost efficiency and flexibility in video content creation. Its innovative multimodal input system is a key highlight, capable of simultaneously accepting text, images, video clips, masks, and various control signals (like human pose, depth maps, etc.).</span></p>
                <div class="meta"><span data-lang-key="article2MetaCategory">Category: Core Features, Workflow Optimization</span> | <span data-lang-key="article2Source">Source: Tongyi Wanxiang Official Announcement</span></div>
            </article>
        </section>

        <section id="core-features-vace" class="mb-12 pt-16 -mt-16">
            <h2 class="section-title" data-lang-key="sectionTitleVaceCore">VACE Core Features: Redefining Video Creation Boundaries</h2>
            <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-6">
                <div class="card">
                    <h3 data-lang-key="vaceFeature1Title">Powerful Controllable Inpainting & Generation</h3>
                    <p data-lang-key="vaceFeature1Desc">Overcoming the pain point of difficult post-generation adjustments in traditional video creation. VACE supports highly controllable video content inpainting and generation based on human pose, motion optical flow, structure preservation, spatial motion paths, video recoloring, and more. It also supports video generation based on reference subject images and background images, ensuring visual element consistency.</p>
                </div>
                <div class="card">
                    <h3 data-lang-key="vaceFeature2Title">Unified & Powerful Multimodal Input System</h3>
                    <p data-lang-key="vaceFeature2Desc">Unlike traditional models that rely solely on text prompts, VACE has built a unified input system that integrates text, images (object reference images or video frames), videos (supporting regeneration after erasure or local extension), Masks (0/1 binary signals to specify editing areas), and various control signals (depth maps, optical flow, layout, grayscale images, line art, poses, etc.).</p>
                </div>
                <div class="card">
                    <h3 data-lang-key="vaceFeature3Title">Precise Spatio-temporal Editing Capabilities</h3>
                    <p data-lang-key="vaceFeature3Desc">VACE empowers users with powerful capabilities for fine-grained video content editing. In the time dimension, it can intelligently complete the entire video duration based on any video segment or just the first and last frames. In the spatial dimension, it supports extended generation for image edges or background areas, such as achieving background replacementâ€”changing the video background environment according to a new Prompt while keeping the main subject animasi.</p>
                </div>
                 <div class="card">
                    <h3 data-lang-key="vaceFeature4Title">Expert-Level Task Handling</h3>
                    <p data-lang-key="vaceFeature4Desc">VACE easily handles complex functions that traditionally require multiple expert models, such as image-referenced generation, video inpainting, and local editing.</p>
                </div>
                <div class="card">
                    <h3 data-lang-key="vaceFeature5Title">Free Combination of Atomic Abilities</h3>
                    <p data-lang-key="vaceFeature5Desc">A revolutionary feature allowing the natural fusion of basic abilities like text-to-video, pose control, background replacement, without needing separate model training for each function.</p>
                </div>
                <div class="card">
                    <h3 data-lang-key="vaceFeature6Title">Multiple Resolution Support</h3>
                    <p data-lang-key="vaceFeature6Desc">The open-sourced VACE-1.3B supports 480P, while VACE-14B supports both 480P and 720P resolutions, catering to various video quality needs.</p>
                </div>
            </div>
        </section>

        <section id="combined-capabilities" class="mb-12 pt-16 -mt-16">
            <div class="card">
                <h2 class="section-title" data-lang-key="sectionTitleCombined">Unlocking Creativity: The Power of Combined Atomic Abilities</h2>
                <p class="mb-4" data-lang-key="combinedIntro">One of VACE's most revolutionary features is its support for the free combination of various single-task capabilities, completely breaking the bottleneck of traditional expert models working in silos and facing collaboration difficulties. As a unified model, VACE can naturally fuse basic (atomic) abilities such as text-to-video generation, pose control, background replacement, and local editing, without the need to train new models separately for a single function. This flexible combination mechanism not only significantly simplifies the video creation workflow but also greatly expands the creative boundaries of AI video generation.</p>
                <p class="mb-2 font-semibold" data-lang-key="combinedExamplesTitle">Examples of Combined Capabilities:</p>
                <ul class="list-disc list-inside ml-4 space-y-2 mb-4">
                    <li><strong class="text-purple-700" data-lang-key="combinedEx1">Object Replacement in Video:</strong> Combine "Image Reference" + "Subject Reshaping" features.</li>
                    <li><strong class="text-purple-700" data-lang-key="combinedEx2">Dynamic Pose Control for Static Images:</strong> Combine "Motion Control" + "First Frame Reference" features.</li>
                    <li><strong class="text-purple-700" data-lang-key="combinedEx3">Expanding a Portrait Image to a Landscape Video with Referenced Elements:</strong> Combine "Image Reference" + "First Frame Reference" + "Background Extension" + "Duration Extension" features.</li>
                </ul>
            </div>
        </section>

        <section id="tech-deep-dive" class="mb-12 pt-16 -mt-16">
            <h2 class="section-title" data-lang-key="sectionTitleTechDeepDive">Tech Deep Dive: The "Magic" Behind VACE</h2>
            <div class="space-y-6">
                <div class="card">
                    <h3 data-lang-key="techVCUTitle">Core Foundation: Unified Input Paradigm â€“ Video Condition Unit (VCU)</h3>
                    <p data-lang-key="techVCUContent">To achieve flexible combination and efficient processing of multiple tasks, the VACE team, after in-depth analysis and summarization of the input forms of four common video tasks (text-to-video, image-to-video, video-to-video, local video-to-video), innovatively proposed a flexible and unified input paradigm: the Video Condition Unit (VCU). The core idea of VCU is to generalize and unify complex multimodal context inputs into three basic forms: Text, Frame Sequence, and Mask Sequence. This design not only unifies the input form for the four core video generation and editing tasks mentioned above, but more crucially, the frame sequences and Mask sequences within VCU can be mathematically overlaid and fused in their representation, creating the necessary conditions for the free combination and synergistic processing of multi-task capabilities later on.</p>
                </div>

                <div class="card">
                    <h3 data-lang-key="techEncodingTitle">Key Step: Unified Encoding of Multimodal Inputs & DiT Integration</h3>
                    <p data-lang-key="techEncodingContent">How to uniformly encode the diverse multimodal inputs within VCU (text, images, videos, Masks, various control signals, etc.) into token sequences that a Diffusion Transformer (DiT) model can efficiently process was a major technical hurdle for VACE. VACE's solution primarily involves the following steps: First, the Frame sequence in the VCU input undergoes conceptual decoupling, categorizing it into two types: one is the RGB pixel part that needs to be preserved verbatim in the generated result (invariant frame sequence), and the other is the content part that needs to be regenerated based on text prompts or other control signals (variable frame sequence). Next, these three types of inputs (variable frames, invariant frames, and Mask) are separately encoded into latent space. Specifically, variable and invariant frames are encoded via a VAE (Variational Autoencoder) into a latent space consistent with the DiT model's noise dimension, with 16 channels. Mask sequences, on the other hand, are mapped to a spatio-temporally consistent latent space feature with 64 channels through elaborate deformation and sampling operations. Finally, the latent space features of the Frame sequences and Mask sequences are effectively combined and then mapped via a set of trainable parameters into token sequences directly processable by the DiT model.</p>
                </div>

                <div class="card">
                    <h3 data-lang-key="techTrainingTitle">Optimization Strategy: Efficient Context Adapter Fine-tuning</h3>
                    <p data-lang-key="techTrainingContent">In selecting the training strategy, the VACE team compared two approaches: Full Fine-tuning and Context Adapter Fine-tuning. Experiments showed that while global fine-tuning (i.e., training all DiT model parameters) can achieve faster inference speeds; however, the context adapter fine-tuning schemeâ€”whose core idea is to fix the parameters of the original base model (e.g., Wan2.1) and only selectively copy and train some of the original Transformer layers as additional adaptersâ€”demonstrated faster convergence. Furthermore, it effectively avoids the risk of the base model's core capabilities suffering from "catastrophic forgetting" or performance degradation during fine-tuning. Therefore, all VACE series models released open-source this time were trained using the context adapter fine-tuning method to ensure model stability and efficiency.</p>
                </div>
            </div>
        </section>

        <section id="performance" class="mb-12 pt-16 -mt-16">
             <div class="card">
                <h2 class="section-title" data-lang-key="sectionTitlePerformance">Performance Evaluation: Significant Improvements in Key Metrics, Outstanding Results</h2>
                <p data-lang-key="performanceContent">Comprehensive quantitative evaluation results for the newly released VACE series models show that, compared to the previous 1.3B preview version, the new models have achieved significant and encouraging improvements across multiple key performance metrics, including the quality of generated video content, controllability of the generation process, and the finesse of editing results. This clearly marks another solid step forward for VACE in its journey towards becoming a more mature and powerful AI video editing and creation tool. (Specific evaluation data charts or comparative examples can be inserted here as appropriate).</p>
             </div>
        </section>

        <section id="getting-started" class="mb-12 pt-16 -mt-16">
            <div class="card bg-purple-50 border-purple-200">
                <h2 class="section-title" data-lang-key="sectionTitleGettingStarted">Get Started Now: Experience and Develop with VACE</h2>
                <p class="mb-4" data-lang-key="gettingStartedIntro">For developers intrigued by the VACE model and eager to experience it or undertake secondary development, you can easily embark on your VACE exploration and innovation journey by following these straightforward steps:</p>
                <ol class="list-decimal list-inside ml-4 space-y-2 mb-6">
                    <li data-lang-key="gettingStartedStep1">Visit the official <strong>GitHub</strong> repository: Download the core source code for Wan2.1.</li>
                    <li data-lang-key="gettingStartedStep2">Obtain Model Weights: Go to the <strong>HuggingFace</strong> community or the domestic <strong>ModelScope (é­”æ­)</strong> platform to download the model weight files corresponding to your chosen VACE version.</li>
                    <li data-lang-key="gettingStartedStep3">Stay Updated with Official Channels: Keep a close watch on the official Tongyi Wanxiang main website, as some user-friendly VACE features and online experience portals will soon be launched, providing you with more support.</li>
                </ol>

                <p class="mb-4 font-semibold" data-lang-key="relevantLinks">Direct Links to Core Relevant Resources:</p>
                <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-4">
                    <a href="https://github.com/Wan-Video/Wan2.1" target="_blank" class="link-button flex items-center justify-center">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" viewBox="0 0 16 16" fill="currentColor"><path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg>
                        GitHub
                    </a>
                    <a href="https://modelscope.cn/organization/Wan-AI" target="_blank" class="link-button flex items-center justify-center">
                        <span class="mr-2 font-bold">é­”</span> ModelScope
                    </a>
                    <a href="https://huggingface.co/Wan-AI" target="_blank" class="link-button flex items-center justify-center">
                        <span class="mr-2 text-2xl">ğŸ¤—</span> Hugging Face
                    </a>
                </div>
                 <p class="mt-6 mb-2 font-semibold" data-lang-key="officialExperience">Official Tongyi Wanxiang Online Experience & Exploration Portals:</p>
                 <div class="grid grid-cols-1 sm:grid-cols-2 gap-4">
                    <a href="https://tongyi.aliyun.com/wanxiang/" target="_blank" class="link-button link-button-secondary flex items-center justify-center" data-lang-key="domesticPortal">Domestic Site (Alibaba Cloud Tongyi)</a>
                    <a href="https://wan.video" target="_blank" class="link-button link-button-secondary flex items-center justify-center" data-lang-key="internationalPortal">International Site (Wan.Video Platform)</a>
                 </div>
            </div>
        </section>
    </main>

    <footer class="bg-gray-900 text-purple-200 py-10 text-center">
        <div class="container mx-auto">
            <p>&copy; <span id="currentYear"></span> <span data-lang-key="footerCopyright">Tongyi Large Model Team. All Rights Reserved.</span></p>
            <p class="text-sm mt-1" data-lang-key="footerDisclaimer">This document aims to provide open-source information about the Tongyi Wanxiang VACE model. For specific model usage, please strictly adhere to the officially released licensing agreements and usage guidelines to ensure compliance.</p>
             <p class="text-sm text-purple-300 mt-2">
                <a href="https://beian.miit.gov.cn/" target="_blank" class="footer-link hover:text-purple-100" data-lang-key="footerICP">æ¸ICPå¤‡2025053282å·</a>
            </p>
        </div>
    </footer>

    <script>
        document.getElementById('currentYear').textContent = new Date().getFullYear();

        const mobileNavToggle = document.getElementById('mobileNavToggle');
        const mainNav = document.querySelector('.main-nav');
        if (mobileNavToggle && mainNav) {
            mobileNavToggle.addEventListener('click', () => {
                mainNav.classList.toggle('open');
            });
        }

        const navLinks = document.querySelectorAll('.nav-link');
        const sections = document.querySelectorAll('main section, section.hero-bg');

        function activateNavLink() {
            let currentSectionId = 'hero-section-for-nav'; // Default to hero
            const headerOffset = document.querySelector('header.main-header')?.offsetHeight || 70;

            sections.forEach(section => {
                const sectionId = section.getAttribute('id');
                if (!sectionId) return; // Skip sections without ID

                const sectionTop = section.offsetTop - headerOffset - 20;
                if (window.pageYOffset >= sectionTop) {
                    currentSectionId = sectionId;
                }
            });

            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === `#${currentSectionId}`) {
                    link.classList.add('active');
                }
            });
        }
        window.addEventListener('scroll', activateNavLink);
        document.addEventListener('DOMContentLoaded', () => {
            applyTranslations();
            activateNavLink();
        });


        navLinks.forEach(link => {
            link.addEventListener('click', (e) => {
                if (mainNav && mainNav.classList.contains('open')) {
                    mainNav.classList.remove('open');
                }
                const targetId = link.getAttribute('href');
                if (targetId.startsWith('#')) {
                    e.preventDefault();
                    const targetElement = document.querySelector(targetId);
                    if (targetElement) {
                        const headerOffset = document.querySelector('header.main-header')?.offsetHeight || 70;
                        const elementPosition = targetElement.getBoundingClientRect().top;
                        const offsetPosition = elementPosition + window.pageYOffset - headerOffset;

                        window.scrollTo({
                            top: offsetPosition,
                            behavior: "smooth"
                        });
                    }
                }
            });
        });

        const translations = {
            "htmlLang": { "zh": "zh-CN", "en": "en" },
            // Header
            "headerVaceTitle": {"zh": "é€šä¹‰ä¸‡ç›¸", "en": "Tongyi Wanxiang"},
            "headerVaceSubtitle": {"zh": "å®£å¸ƒå¼€æºè§†é¢‘ç¼–è¾‘åˆ©å™¨", "en": "Announcing the Open Source Video Editing Powerhouse"},
            // Hero
            "heroTitle": { "zh": "é€šä¹‰ä¸‡ç›¸ VACE (Wan2.1) éœ‡æ’¼å¼€æºï¼", "en": "VACE (Wan2.1) by Tongyi Wanxiang: Open Source Now!" },
            "heroSubtitle": { "zh": "ä¸€æ¬¾æ¨¡å‹æå®šå¤šç§å¤æ‚è§†é¢‘ç¼–è¾‘ä»»åŠ¡ï¼Œé¢ è¦†ä¼ ç»Ÿåˆ›ä½œæµç¨‹ã€‚", "en": "One model to master diverse video editing tasks, revolutionizing creative workflows." },
            "heroCTA": { "zh": "æ¢ç´¢VACE &raquo;", "en": "Explore VACE &raquo;" },
            // Nav
            "navLatestUpdates": { "zh": "æœ€æ–°åŠ¨æ€", "en": "Latest Updates" },
            "navCoreFeatures": { "zh": "æ ¸å¿ƒç‰¹æ€§", "en": "Core Features" },
            "navCombinedCapabilities": { "zh": "ç»„åˆèƒ½åŠ›", "en": "Combined Capabilities"},
            "navTechDeepDive": { "zh": "æŠ€æœ¯æ¢ç§˜", "en": "Tech Deep Dive"},
            "navGettingStarted": { "zh": "å¿«é€Ÿä¸Šæ‰‹", "en": "Get Started"},
            "navAnalysisInsights": { "zh": "åˆ†æè§£è¯»", "en": "Analysis & Insights" },

            // Latest Updates Section
            "sectionTitleLatestUpdates": { "zh": "VACE å¼€æºæœ€æ–°åŠ¨æ€", "en": "Latest Updates on VACE Open Source" },
            "article1Title": { "zh": "é€šä¹‰ä¸‡ç›¸VACE (Wan2.1)å¼€æºï¼šä¸€æ¬¾ç»Ÿä¸€æ¨¡å‹èµ‹èƒ½å¤šæ ·åŒ–è§†é¢‘ç¼–è¾‘", "en": "Tongyi Wanxiang VACE (Wan2.1) Open Sourced: A Unified Model for Diverse Video Editing" },
            "article1Date": { "zh": "[2025å¹´5æœˆ15æ—¥]", "en": "[May 15, 2025]" },
            "article1Intro": { "zh": "é˜¿é‡Œå·´å·´é€šä¹‰ä¸‡ç›¸å›¢é˜Ÿæ­£å¼å¼€æºå…¶é‡Œç¨‹ç¢‘å¼çš„VACE (è§†é¢‘åˆ›ä½œä¸ç¼–è¾‘å¼•æ“)æ¨¡å‹ï¼Œä½œä¸ºWan2.1ç³»åˆ—çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚VACEè‡´åŠ›äºé€šè¿‡å•ä¸€çš„å¼ºå¤§æ¨¡å‹ï¼Œæä¾›ä¸€ç«™å¼ã€é«˜æ•ˆä¸”çµæ´»çš„è§†é¢‘åˆ›ä½œä¸ç¼–è¾‘è§£å†³æ–¹æ¡ˆï¼Œè§£å†³ä»¥å¾€éœ€è¦åˆ‡æ¢å¤šç§å·¥å…·çš„ç—›ç‚¹ã€‚æœ¬æ¬¡å¼€æºåŒ…å«æ”¯æŒ480Påˆ†è¾¨ç‡çš„VACE-1.3Bç‰ˆæœ¬ä»¥åŠæ”¯æŒ480Pä¸720Pçš„VACE-14Bç‰ˆæœ¬ã€‚", "en": "Alibaba Cloud's Tongyi Wanxiang team has officially open-sourced its groundbreaking VACE (Video Anymator and Composer Engine) model, a core component of the Wan2.1 series. VACE aims to provide a one-stop, efficient, and flexible video creation and editing experience by consolidating multiple complex tasks into a single, powerful model. This release includes VACE-1.3B supporting 480P resolution and VACE-14B supporting both 480P and 720P." },
            "article1MetaCategory": { "zh": "åˆ†ç±»ï¼šæœ€æ–°åŠ¨æ€, æ¨¡å‹å‘å¸ƒ", "en": "Category: Latest Updates, Model Release" },
            "article1ReadMore": { "zh": "äº†è§£æ›´å¤šæŠ€æœ¯ç»†èŠ‚ &raquo;", "en": "Learn More Technical Details &raquo;" },
            "timelineTitle": { "zh": "å…³é”®é‡Œç¨‹ç¢‘:", "en": "Key Milestones:" },
            "timelineItem1": { "zh": "<strong>Wan2.1ç³»åˆ—å‘å¸ƒ:</strong> VACEä½œä¸ºæ ¸å¿ƒç»„ä»¶ä¸€åŒäº®ç›¸ï¼Œå±•ç¤ºå…¶ç»Ÿä¸€è§†é¢‘å¤„ç†èƒ½åŠ›ã€‚", "en": "<strong>Wan2.1 Series Release:</strong> VACE unveiled as a core component, showcasing its unified video processing capabilities." },
            "timelineItem2": { "zh": "<strong>VACE 1.3B & 14B æ¨¡å‹å¼€æº:</strong> å‘ç¤¾åŒºæä¾›æ”¯æŒä¸åŒåˆ†è¾¨ç‡ï¼ˆ480P/720Pï¼‰çš„å¼ºå¤§æ¨¡å‹ã€‚", "en": "<strong>VACE 1.3B & 14B Models Open Sourced:</strong> Powerful models supporting different resolutions (480P/720P) released to the community." },
            "timelineItem3": { "zh": "<strong>ç¤¾åŒºç§¯æåé¦ˆä¸æ¢ç´¢:</strong> å¼€å‘è€…å¼€å§‹åŸºäºVACEè¿›è¡ŒäºŒæ¬¡åˆ›ä½œã€è¯„ä¼°ï¼Œå¹¶æ¢ç´¢å…¶åœ¨å„è¡Œå„ä¸šçš„åº”ç”¨æ½œåŠ›ã€‚", "en": "<strong>Active Community Feedback & Exploration:</strong> Developers begin secondary creation, evaluation, and exploring VACE's application potential across various industries." },

            "article2Title": { "zh": "ä¸€ä¸ªæ¨¡å‹ï¼Œå¤šç§ä»»åŠ¡ï¼šVACEå¦‚ä½•ç®€åŒ–å¤æ‚è§†é¢‘ç¼–è¾‘", "en": "One Model, Multiple Tasks: How VACE Simplifies Complex Video Editing" },
            "article2Date": { "zh": "[2025å¹´5æœˆ15æ—¥]", "en": "[May 15, 2025]" },
            "article2Content": { "zh": "å€ŸåŠ©VACEï¼Œç”¨æˆ·ç°åœ¨å¯ä»¥æ— ç¼æ‰§è¡Œæ–‡ç”Ÿè§†é¢‘ã€å›¾åƒå‚è€ƒç”Ÿæˆã€è§†é¢‘å±€éƒ¨ç¼–è¾‘ä»¥åŠè§†é¢‘æ—¶é•¿ä¸ç©ºé—´æ‰©å±•ç­‰å¤šç§ä»»åŠ¡ï¼Œæ— éœ€åœ¨ä¸åŒçš„æ¨¡å‹æˆ–å·¥å…·é—´é¢‘ç¹åˆ‡æ¢ã€‚è¿™ç§ç»Ÿä¸€çš„æ–¹æ³•æœ‰æœ›æ˜¾è‘—æå‡è§†é¢‘å†…å®¹åˆ›ä½œçš„æ•ˆç‡å’Œçµæ´»æ€§ã€‚å…¶åˆ›æ–°çš„å¤šæ¨¡æ€è¾“å…¥ç³»ç»Ÿæ˜¯æ ¸å¿ƒäº®ç‚¹ï¼Œèƒ½å¤ŸåŒæ—¶æ¥å—æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ç‰‡æ®µã€è’™ç‰ˆ(Mask)ä»¥åŠå¤šç§æ§åˆ¶ä¿¡å·ï¼ˆå¦‚äººä½“å§¿æ€ã€æ·±åº¦å›¾ç­‰ï¼‰ã€‚", "en": "With VACE, users can now seamlessly perform text-to-video generation, image-referenced generation, local video editing, and video duration/spatial extension tasks without frequently switching between different models or tools. This unified approach is set to significantly boost efficiency and flexibility in video content creation. Its innovative multimodal input system is a key highlight, capable of simultaneously accepting text, images, video clips, masks, and various control signals (like human pose, depth maps, etc.)." },
            "article2MetaCategory": { "zh": "åˆ†ç±»ï¼šæ ¸å¿ƒåŠŸèƒ½, å·¥ä½œæµç¨‹ä¼˜åŒ–", "en": "Category: Core Features, Workflow Optimization" },
            "article2Source": { "zh": "æ¥æº: é€šä¹‰ä¸‡ç›¸å®˜æ–¹å…¬å‘Š", "en": "Source: Tongyi Wanxiang Official Announcement" },
             "article2Published": { "zh": "å‘å¸ƒäº: 3å°æ—¶å‰", "en": "Published: 3 hours ago" },

            // Core Features Section
            "sectionTitleVaceCore": { "zh": "VACE æ ¸å¿ƒç‰¹æ€§ï¼šé‡å¡‘è§†é¢‘åˆ›ä½œè¾¹ç•Œ", "en": "VACE Core Features: Redefining Video Creation Boundaries" },
            "vaceFeature1Title": { "zh": "å¼ºå¤§çš„å¯æ§é‡ç»˜ä¸ç”Ÿæˆ", "en": "Powerful Controllable Inpainting & Generation" },
            "vaceFeature1Desc": { "zh": "é¢ è¦†ä¼ ç»Ÿè§†é¢‘ç”Ÿæˆåè°ƒæ•´å›°éš¾çš„ç—›ç‚¹ã€‚VACEæ”¯æŒåŸºäºäººä½“å§¿æ€ã€è¿åŠ¨å…‰æµã€ç»“æ„ä¿æŒã€ç©ºé—´è¿åŠ¨è½¨è¿¹ã€è§†é¢‘é‡æ–°ç€è‰²ç­‰å¤šç§æ–¹å¼è¿›è¡Œé«˜åº¦å¯æ§çš„è§†é¢‘å†…å®¹é‡ç»˜ä¸ç”Ÿæˆã€‚åŒæ—¶ï¼Œä¹Ÿæ”¯æŒåŸºäºå‚è€ƒä¸»ä½“å›¾åƒå’ŒèƒŒæ™¯å›¾åƒçš„è§†é¢‘ç”Ÿæˆï¼Œç¡®ä¿è§†è§‰å…ƒç´ çš„ä¸€è‡´æ€§ã€‚", "en": "Overcoming the pain point of difficult post-generation adjustments in traditional video creation. VACE supports highly controllable video content inpainting and generation based on human pose, motion optical flow, structure preservation, spatial motion paths, video recoloring, and more. It also supports video generation based on reference subject images and background images, ensuring visual element consistency." },
            "vaceFeature2Title": { "zh": "ç»Ÿä¸€ä¸”å¼ºå¤§çš„å¤šæ¨¡æ€è¾“å…¥ç³»ç»Ÿ", "en": "Unified & Powerful Multimodal Input System" },
            "vaceFeature2Desc": { "zh": "ä¸ä»…ä¾èµ–æ–‡æœ¬æç¤ºçš„ä¼ ç»Ÿæ¨¡å‹ä¸åŒï¼ŒVACEæ„å»ºäº†ä¸€ä¸ªé›†æ–‡æœ¬ã€å›¾åƒï¼ˆç‰©ä½“å‚è€ƒå›¾æˆ–è§†é¢‘å¸§ï¼‰ã€è§†é¢‘ï¼ˆæ”¯æŒæŠ¹é™¤ã€å±€éƒ¨æ‰©å±•åé‡ç”Ÿæˆï¼‰ã€Maskï¼ˆ0/1äºŒå€¼ä¿¡å·æŒ‡å®šç¼–è¾‘åŒºåŸŸï¼‰ä»¥åŠå¤šç§æ§åˆ¶ä¿¡å·ï¼ˆæ·±åº¦å›¾ã€å…‰æµã€å¸ƒå±€ã€ç°åº¦å›¾ã€çº¿ç¨¿ã€å§¿æ€ç­‰ï¼‰äºä¸€ä½“çš„ç»Ÿä¸€è¾“å…¥ç³»ç»Ÿã€‚", "en": "Unlike traditional models that rely solely on text prompts, VACE has built a unified input system that integrates text, images (object reference images or video frames), videos (supporting regeneration after erasure or local extension), Masks (0/1 binary signals to specify editing areas), and various control signals (depth maps, optical flow, layout, grayscale images, line art, poses, etc.)." },
            "vaceFeature3Title": { "zh": "ç²¾å‡†çš„æ—¶ç©ºç»´åº¦ç¼–è¾‘èƒ½åŠ›", "en": "Precise Spatio-temporal Editing Capabilities" },
            "vaceFeature3Desc": { "zh": "VACEèµ‹äºˆç”¨æˆ·å¯¹è§†é¢‘å†…å®¹è¿›è¡Œç²¾ç»†åŒ–ç¼–è¾‘çš„å¼ºå¤§èƒ½åŠ›ã€‚åœ¨æ—¶é—´ç»´åº¦ä¸Šï¼Œå¯æ ¹æ®ä»»æ„è§†é¢‘ç‰‡æ®µæˆ–ä»…é¦–å°¾å¸§æ™ºèƒ½è¡¥å…¨æ•´ä¸ªè§†é¢‘æ—¶é•¿ï¼›åœ¨ç©ºé—´ç»´åº¦ä¸Šï¼Œæ”¯æŒå¯¹ç”»é¢è¾¹ç¼˜æˆ–èƒŒæ™¯åŒºåŸŸè¿›è¡Œæ‰©å±•ç”Ÿæˆï¼Œä¾‹å¦‚å®ç°èƒŒæ™¯æ›¿æ¢â€”â€”åœ¨ä¿ç•™ä¸»ä½“ä¸å˜çš„å‰æä¸‹ï¼Œä¾æ®æ–°çš„Promptæ›´æ¢è§†é¢‘èƒŒæ™¯ç¯å¢ƒã€‚", "en": "VACE empowers users with powerful capabilities for fine-grained video content editing. In the time dimension, it can intelligently complete the entire video duration based on any video segment or just the first and last frames. In the spatial dimension, it supports extended generation for image edges or background areas, such as achieving background replacementâ€”changing the video background environment according to a new Prompt while keeping the main subject animasi." },
            "vaceFeature4Title": { "zh": "é©¾é©­ä¸“å®¶çº§ä»»åŠ¡", "en": "Expert-Level Task Handling" },
            "vaceFeature4Desc": { "zh": "VACEèƒ½è½»æ¾èƒœä»»ä¼ ç»Ÿéœ€å¤šä¸ªä¸“å®¶æ¨¡å‹åä½œå®Œæˆçš„å¤æ‚åŠŸèƒ½ï¼Œå¦‚å›¾åƒå‚è€ƒç”Ÿæˆã€è§†é¢‘é‡ç»˜ã€å±€éƒ¨ç¼–è¾‘ç­‰ã€‚", "en": "VACE easily handles complex functions that traditionally require multiple expert models, such as image-referenced generation, video inpainting, and local editing." },
            "vaceFeature5Title": { "zh": "åŸå­èƒ½åŠ›çš„è‡ªç”±ç»„åˆ", "en": "Free Combination of Atomic Abilities" },
            "vaceFeature5Desc": { "zh": "é©å‘½æ€§ç‰¹æ€§ï¼Œå…è®¸è‡ªç„¶èåˆæ–‡ç”Ÿè§†é¢‘ã€å§¿æ€æ§åˆ¶ã€èƒŒæ™¯æ›¿æ¢ç­‰åŸºç¡€èƒ½åŠ›ï¼Œæ— éœ€ä¸ºå•ä¸€åŠŸèƒ½å•ç‹¬è®­ç»ƒæ–°æ¨¡å‹ã€‚", "en": "A revolutionary feature allowing the natural fusion of basic abilities like text-to-video, pose control, background replacement, without needing separate model training for each function." },
            "vaceFeature6Title": { "zh": "å¤šåˆ†è¾¨ç‡æ”¯æŒ", "en": "Multiple Resolution Support" },
            "vaceFeature6Desc": { "zh": "å¼€æºçš„VACE-1.3Bæ”¯æŒ480Pï¼ŒVACE-14Båˆ™åŒæ—¶æ”¯æŒ480På’Œ720Påˆ†è¾¨ç‡ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯å¯¹è§†é¢‘è´¨é‡çš„éœ€æ±‚ã€‚", "en": "The open-sourced VACE-1.3B supports 480P, while VACE-14B supports both 480P and 720P resolutions, catering to various video quality needs." },

            // Combined Capabilities Section
            "sectionTitleCombined": { "zh": "é‡Šæ”¾åˆ›æ„ï¼šåŸå­èƒ½åŠ›çš„è‡ªç”±ç»„åˆå¨åŠ›", "en": "Unlocking Creativity: The Power of Combined Atomic Abilities" },
            "combinedIntro": { "zh": "VACEæœ€å…·é©å‘½æ€§çš„ç‰¹ç‚¹ä¹‹ä¸€æ˜¯å…¶æ”¯æŒå¤šç§å•ä»»åŠ¡èƒ½åŠ›çš„è‡ªç”±ç»„åˆï¼Œå½»åº•æ‰“ç ´äº†ä¼ ç»Ÿä¸“å®¶æ¨¡å‹å„è‡ªä¸ºæˆ˜ã€åä½œå›°éš¾çš„ç“¶é¢ˆã€‚ä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹ï¼ŒVACEèƒ½å¤Ÿè‡ªç„¶åœ°èåˆæ–‡æœ¬ç”Ÿæˆè§†é¢‘ã€å§¿æ€æ§åˆ¶ã€èƒŒæ™¯æ›¿æ¢ã€å±€éƒ¨ç¼–è¾‘ç­‰åŸºç¡€ï¼ˆåŸå­ï¼‰èƒ½åŠ›ï¼Œæ— éœ€ä¸ºå®ç°æŸä¸€å•ä¸€åŠŸèƒ½è€Œå•ç‹¬è®­ç»ƒæ–°æ¨¡å‹ã€‚è¿™ç§çµæ´»çš„ç»„åˆæœºåˆ¶ï¼Œä¸ä»…å¤§å¹…ç®€åŒ–äº†è§†é¢‘åˆ›ä½œæµç¨‹ï¼Œä¹Ÿæå¤§åœ°æ‹“å±•äº†AIè§†é¢‘ç”Ÿæˆçš„åˆ›æ„è¾¹ç•Œã€‚", "en": "One of VACE's most revolutionary features is its support for the free combination of various single-task capabilities, completely breaking the bottleneck of traditional expert models working in silos and facing collaboration difficulties. As a unified model, VACE can naturally fuse basic (atomic) abilities such as text-to-video generation, pose control, background replacement, and local editing, without the need to train new models separately for a single function. This flexible combination mechanism not only significantly simplifies the video creation workflow but also greatly expands the creative boundaries of AI video generation." },
            "combinedExamplesTitle": { "zh": "ç»„åˆèƒ½åŠ›ç¤ºä¾‹ï¼š", "en": "Examples of Combined Capabilities:" },
            "combinedEx1": { "zh": "<strong>è§†é¢‘ä¸­ç‰©ä½“æ›¿æ¢ï¼š</strong> ç»„åˆâ€œå›¾ç‰‡å‚è€ƒâ€ + â€œä¸»ä½“é‡å¡‘â€åŠŸèƒ½ã€‚", "en": "<strong>Object Replacement in Video:</strong> Combine \"Image Reference\" + \"Subject Reshaping\" features." },
            "combinedEx2": { "zh": "<strong>é™æ€å›¾ç‰‡åŠ¨æ€å§¿æ€æ§åˆ¶ï¼š</strong> ç»„åˆâ€œè¿åŠ¨æ§åˆ¶â€ + â€œé¦–å¸§å‚è€ƒâ€åŠŸèƒ½ã€‚", "en": "<strong>Dynamic Pose Control for Static Images:</strong> Combine \"Motion Control\" + \"First Frame Reference\" features." },
            "combinedEx3": { "zh": "<strong>ç«–ç‰ˆå›¾æ‹“å±•ä¸ºæ¨ªç‰ˆè§†é¢‘å¹¶åŠ å…¥å‚è€ƒå…ƒç´ ï¼š</strong> ç»„åˆâ€œå›¾ç‰‡å‚è€ƒâ€ + â€œé¦–å¸§å‚è€ƒâ€ + â€œèƒŒæ™¯æ‰©å±•â€ + â€œæ—¶é•¿å»¶å±•â€åŠŸèƒ½ã€‚", "en": "<strong>Expanding a Portrait Image to a Landscape Video with Referenced Elements:</strong> Combine \"Image Reference\" + \"First Frame Reference\" + \"Background Extension\" + \"Duration Extension\" features." },

            // Tech Deep Dive Section
            "sectionTitleTechDeepDive": { "zh": "æŠ€æœ¯æ¢ç§˜ï¼šVACE èƒŒåçš„â€œé­”æ³•â€", "en": "Tech Deep Dive: The \"Magic\" Behind VACE" },
            "techVCUTitle": { "zh": "æ ¸å¿ƒåŸºçŸ³ï¼šç»Ÿä¸€è¾“å…¥èŒƒå¼â€”â€”è§†é¢‘æ¡ä»¶å•å…ƒ (VCU)", "en": "Core Foundation: Unified Input Paradigm â€“ Video Condition Unit (VCU)" },
            "techVCUContent": { "zh": "ä¸ºäº†å®ç°å¤šä»»åŠ¡çš„çµæ´»ç»„åˆå’Œé«˜æ•ˆå¤„ç†ï¼ŒVACEå›¢é˜Ÿåœ¨å¯¹å››ç±»å¸¸è§è§†é¢‘ä»»åŠ¡ï¼ˆæ–‡ç”Ÿè§†é¢‘ã€å›¾ç”Ÿè§†é¢‘ã€è§†é¢‘ç”Ÿè§†é¢‘ã€å±€éƒ¨è§†é¢‘ç”Ÿè§†é¢‘ï¼‰çš„è¾“å…¥å½¢æ€è¿›è¡Œæ·±å…¥åˆ†æå’Œæ€»ç»“åï¼Œåˆ›æ–°æ€§åœ°æå‡ºäº†ä¸€ä¸ªçµæ´»ä¸”ç»Ÿä¸€çš„è¾“å…¥èŒƒå¼ï¼šè§†é¢‘æ¡ä»¶å•å…ƒï¼ˆVideo Condition Unit, VCUï¼‰ã€‚VCUçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å¤æ‚çš„å¤šæ¨¡æ€ä¸Šä¸‹æ–‡è¾“å…¥ï¼Œå½’çº³å¹¶ç»Ÿä¸€ä¸ºä¸‰ç§åŸºæœ¬å½¢æ€ï¼šæ–‡æœ¬ï¼ˆTextï¼‰ã€å¸§åºåˆ—ï¼ˆFrame Sequenceï¼‰ã€ä»¥åŠMaskåºåˆ—ï¼ˆMask Sequenceï¼‰ã€‚è¿™ç§è®¾è®¡ä¸ä»…åœ¨è¾“å…¥å½¢å¼ä¸Šç»Ÿä¸€äº†ä¸Šè¿°å››ç±»æ ¸å¿ƒçš„è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ï¼Œæ›´ä¸ºå…³é”®çš„æ˜¯ï¼ŒVCUä¸­çš„å¸§åºåˆ—å’ŒMaskåºåˆ—åœ¨æ•°å­¦è¡¨å¾ä¸Šå¯ä»¥ç›¸äº’å åŠ å’Œèåˆï¼Œä¸ºåç»­å¤šä»»åŠ¡èƒ½åŠ›çš„è‡ªç”±ç»„åˆä¸ååŒå¤„ç†åˆ›é€ äº†å¿…è¦æ¡ä»¶ã€‚", "en": "To achieve flexible combination and efficient processing of multiple tasks, the VACE team, after in-depth analysis and summarization of the input forms of four common video tasks (text-to-video, image-to-video, video-to-video, local video-to-video), innovatively proposed a flexible and unified input paradigm: the Video Condition Unit (VCU). The core idea of VCU is to generalize and unify complex multimodal context inputs into three basic forms: Text, Frame Sequence, and Mask Sequence. This design not only unifies the input form for the four core video generation and editing tasks mentioned above, but more crucially, the frame sequences and Mask sequences within VCU can be mathematically overlaid and fused in their representation, creating the necessary conditions for the free combination and synergistic processing of multi-task capabilities later on." },
            "techEncodingTitle": { "zh": "å…³é”®ç¯èŠ‚ï¼šå¤šæ¨¡æ€è¾“å…¥çš„ç»Ÿä¸€ç¼–ç ä¸DiTæ•´åˆ", "en": "Key Step: Unified Encoding of Multimodal Inputs & DiT Integration" },
            "techEncodingContent": { "zh": "å¦‚ä½•å°†VCUä¸­å¤šæ ·çš„å¤šæ¨¡æ€è¾“å…¥ï¼ˆæ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ã€Maskã€å„ç±»æ§åˆ¶ä¿¡å·ç­‰ï¼‰ç»Ÿä¸€ç¼–ç ä¸ºæ‰©æ•£Transformerï¼ˆDiTï¼‰æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå¤„ç†çš„tokenåºåˆ—ï¼Œæ˜¯VACEéœ€è¦æ”»å…‹çš„ä¸€å¤§æŠ€æœ¯éš¾é¢˜ã€‚VACEçš„è§£å†³æ–¹æ¡ˆä¸»è¦åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼šé¦–å…ˆï¼Œå¯¹VCUè¾“å…¥ä¸­çš„Frameåºåˆ—è¿›è¡Œæ¦‚å¿µè§£è€¦ï¼Œå°†å…¶ç»†åˆ†ä¸ºä¸¤ç±»ï¼šä¸€ç±»æ˜¯éœ€è¦åœ¨ç”Ÿæˆç»“æœä¸­åŸå°ä¸åŠ¨ä¿ç•™çš„RGBåƒç´ éƒ¨åˆ†ï¼ˆä¸å˜å¸§åºåˆ—ï¼‰ï¼Œå¦ä¸€ç±»æ˜¯éœ€è¦æ ¹æ®æ–‡æœ¬æç¤ºæˆ–å…¶ä»–æ§åˆ¶ä¿¡å·é‡æ–°ç”Ÿæˆçš„å†…å®¹éƒ¨åˆ†ï¼ˆå¯å˜å¸§åºåˆ—ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œåˆ†åˆ«å¯¹è¿™ä¸‰ç±»è¾“å…¥ï¼ˆå¯å˜å¸§ã€ä¸å˜å¸§ã€Maskï¼‰è¿›è¡Œéšç©ºé—´ç¼–ç ã€‚å…¶ä¸­ï¼Œå¯å˜å¸§å’Œä¸å˜å¸§é€šè¿‡VAEï¼ˆVariational Autoencoderï¼‰è¢«ç¼–ç åˆ°ä¸DiTæ¨¡å‹å™ªå£°ç»´åº¦ä¸€è‡´çš„éšç©ºé—´ï¼Œé€šé“æ•°ä¸º16ï¼›è€ŒMaskåºåˆ—åˆ™é€šè¿‡ç²¾å·§çš„å˜å½¢ï¼ˆdeformationï¼‰å’Œé‡‡æ ·ï¼ˆsamplingï¼‰æ“ä½œï¼Œè¢«æ˜ å°„åˆ°æ—¶ç©ºç»´åº¦ä¸€è‡´ã€é€šé“æ•°ä¸º64çš„éšç©ºé—´ç‰¹å¾ã€‚æœ€åï¼Œå°†Frameåºåˆ—å’ŒMaskåºåˆ—çš„éšç©ºé—´ç‰¹å¾è¿›è¡Œæœ‰æ•ˆåˆå¹¶ï¼Œå¹¶é€šè¿‡ä¸€ç»„å¯è®­ç»ƒçš„å‚æ•°å°†å…¶æ˜ å°„ä¸ºDiTæ¨¡å‹å¯ç›´æ¥å¤„ç†çš„tokenåºåˆ—ã€‚", "en": "How to uniformly encode the diverse multimodal inputs within VCU (text, images, videos, Masks, various control signals, etc.) into token sequences that a Diffusion Transformer (DiT) model can efficiently process was a major technical hurdle for VACE. VACE's solution primarily involves the following steps: First, the Frame sequence in the VCU input undergoes conceptual decoupling, categorizing it into two types: one is the RGB pixel part that needs to be preserved verbatim in the generated result (invariant frame sequence), and the other is the content part that needs to be regenerated based on text prompts or other control signals (variable frame sequence). Next, these three types of inputs (variable frames, invariant frames, and Mask) are separately encoded into latent space. Specifically, variable and invariant frames are encoded via a VAE (Variational Autoencoder) into a latent space consistent with the DiT model's noise dimension, with 16 channels. Mask sequences, on the other hand, are mapped to a spatio-temporally consistent latent space feature with 64 channels through elaborate deformation and sampling operations. Finally, the latent space features of the Frame sequences and Mask sequences are effectively combined and then mapped via a set of trainable parameters into token sequences directly processable by the DiT model." },
            "techTrainingTitle": { "zh": "ä¼˜åŒ–ç­–ç•¥ï¼šé«˜æ•ˆçš„ä¸Šä¸‹æ–‡é€‚é…å™¨å¾®è°ƒè®­ç»ƒ", "en": "Optimization Strategy: Efficient Context Adapter Fine-tuning" },
            "techTrainingContent": { "zh": "åœ¨è®­ç»ƒç­–ç•¥çš„é€‰æ‹©ä¸Šï¼ŒVACEå›¢é˜Ÿå¯¹æ¯”äº†å…¨å±€å¾®è°ƒï¼ˆFull Fine-tuningï¼‰ä¸ä¸Šä¸‹æ–‡é€‚é…å™¨å¾®è°ƒï¼ˆContext Adapter Fine-tuningï¼‰ä¸¤ç§æ–¹æ¡ˆã€‚å®éªŒè¡¨æ˜ï¼Œè™½ç„¶å…¨å±€å¾®è°ƒï¼ˆå³è®­ç»ƒå…¨éƒ¨DiTæ¨¡å‹å‚æ•°ï¼‰èƒ½å¤Ÿå–å¾—æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œä½†ä¸Šä¸‹æ–‡é€‚é…å™¨å¾®è°ƒæ–¹æ¡ˆâ€”â€”å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å›ºå®šåŸå§‹åŸºæ¨¡å‹ï¼ˆå¦‚Wan2.1ï¼‰çš„å‚æ•°ï¼Œä»…é€‰æ‹©æ€§åœ°å¤åˆ¶å¹¶è®­ç»ƒä¸€éƒ¨åˆ†åŸå§‹Transformerå±‚ä½œä¸ºé¢å¤–çš„é€‚é…å™¨â€”â€”å±•ç°å‡ºäº†æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼Œå¹¶ä¸”èƒ½æœ‰æ•ˆé¿å…åŸºç¡€æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­å‘ç”Ÿâ€œç¾éš¾æ€§é—å¿˜â€æˆ–æ€§èƒ½è¡°é€€çš„é£é™©ã€‚å› æ­¤ï¼Œæœ¬æ¬¡å¼€æºå‘å¸ƒçš„VACEç³»åˆ—æ¨¡å‹å‡é‡‡ç”¨äº†ä¸Šä¸‹æ–‡é€‚é…å™¨å¾®è°ƒæ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œä»¥ç¡®ä¿æ¨¡å‹çš„ç¨³å®šæ€§å’Œé«˜æ•ˆæ€§ã€‚", "en": "In selecting the training strategy, the VACE team compared two approaches: Full Fine-tuning and Context Adapter Fine-tuning. Experiments showed that while global fine-tuning (i.e., training all DiT model parameters) can achieve faster inference speeds; however, the context adapter fine-tuning schemeâ€”whose core idea is to fix the parameters of the original base model (e.g., Wan2.1) and only selectively copy and train some of the original Transformer layers as additional adaptersâ€”demonstrated faster convergence. Furthermore, it effectively avoids the risk of the base model's core capabilities suffering from "catastrophic forgetting" or performance degradation during fine-tuning. Therefore, all VACE series models released open-source this time were trained using the context adapter fine-tuning method to ensure model stability and efficiency." },

            // Performance Section
            "sectionTitlePerformance": { "zh": "æ€§èƒ½è¯„ä¼°ï¼šå…³é”®æŒ‡æ ‡æ˜¾è‘—æå‡ï¼Œæ•ˆæœå“è¶Š", "en": "Performance Evaluation: Significant Improvements in Key Metrics, Outstanding Results" },
            "performanceContent": { "zh": "é€šè¿‡å¯¹æœ¬æ¬¡å‘å¸ƒçš„VACEç³»åˆ—æ¨¡å‹è¿›è¡Œçš„å…¨é¢å®šé‡è¯„æµ‹ç»“æœæ˜¾ç¤ºï¼Œç›¸è¾ƒäºæ­¤å‰çš„1.3B previewç‰ˆæœ¬ï¼Œæ–°æ¨¡å‹åœ¨è§†é¢‘ç”Ÿæˆå†…å®¹çš„è´¨é‡ã€ç”Ÿæˆè¿‡ç¨‹çš„å¯æ§æ€§ä»¥åŠç¼–è¾‘ç»“æœçš„ç²¾ç»†åº¦ç­‰å¤šä¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡ä¸Šå‡å–å¾—äº†æ˜¾è‘—ä¸”ä»¤äººé¼“èˆçš„æå‡ã€‚è¿™æ¸…æ™°åœ°æ ‡å¿—ç€VACEåœ¨å‘ä¸€ä¸ªæ›´ä¸ºæˆç†Ÿã€åŠŸèƒ½æ›´ä¸ºå¼ºå¤§çš„AIè§†é¢‘ç¼–è¾‘ä¸åˆ›ä½œå·¥å…·è¿ˆè¿›çš„è¿‡ç¨‹ä¸­ï¼Œåˆå–å¾—äº†åšå®çš„ä¸€å¤§æ­¥ã€‚ï¼ˆæ­¤å¤„å¯æ ¹æ®å®é™…æƒ…å†µï¼Œæ’å…¥å…·ä½“çš„è¯„æµ‹æ•°æ®å›¾è¡¨æˆ–å¯¹æ¯”æ¡ˆä¾‹è¿›è¡Œå±•ç¤ºï¼‰", "en": "Comprehensive quantitative evaluation results for the newly released VACE series models show that, compared to the previous 1.3B preview version, the new models have achieved significant and encouraging improvements across multiple key performance metrics, including the quality of generated video content, controllability of the generation process, and the finesse of editing results. This clearly marks another solid step forward for VACE in its journey towards becoming a more mature and powerful AI video editing and creation tool. (Specific evaluation data charts or comparative examples can be inserted here as appropriate)." },

            // Getting Started Section
            "sectionTitleGettingStarted": { "zh": "å³åˆ»ä¸Šæ‰‹ï¼šä½“éªŒä¸äºŒæ¬¡å¼€å‘VACE", "en": "Get Started Now: Experience and Develop with VACE" },
            "gettingStartedIntro": { "zh": "å¯¹VACEæ¨¡å‹å……æ»¡å…´è¶£å¹¶å¸Œæœ›ç«‹å³ä½“éªŒæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘çš„å¼€å‘è€…ä»¬ï¼Œå¯ä»¥éµå¾ªä»¥ä¸‹ç®€æ´æ˜äº†çš„æ­¥éª¤ï¼Œè½»æ¾å¼€å¯æ‚¨çš„VACEæ¢ç´¢ä¸åˆ›æ–°ä¹‹æ—…ï¼š", "en": "For developers intrigued by the VACE model and eager to experience it or undertake secondary development, you can easily embark on your VACE exploration and innovation journey by following these straightforward steps:" },
            "gettingStartedStep1": { "zh": "è®¿é—®å®˜æ–¹ <strong>GitHub</strong> ä»£ç ä»“åº“ï¼šä¸‹è½½ Wan2.1 çš„æ ¸å¿ƒæºä»£ç ã€‚", "en": "Visit the official <strong>GitHub</strong> repository: Download the core source code for Wan2.1." },
            "gettingStartedStep2": { "zh": "è·å–æ¨¡å‹æƒé‡ï¼šå‰å¾€ <strong>HuggingFace</strong> ç¤¾åŒºæˆ–å›½å†…çš„ <strong>ModelScope (é­”æ­)</strong> å¹³å°ï¼Œä¸‹è½½ä¸æ‚¨é€‰æ‹©çš„VACEç‰ˆæœ¬ç›¸å¯¹åº”çš„æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚", "en": "Obtain Model Weights: Go to the <strong>HuggingFace</strong> community or the domestic <strong>ModelScope (é­”æ­)</strong> platform to download the model weight files corresponding to your chosen VACE version." },
            "gettingStartedStep3": { "zh": "å…³æ³¨å®˜æ–¹åŠ¨æ€ï¼šå¯†åˆ‡å…³æ³¨é€šä¹‰ä¸‡ç›¸çš„å®˜æ–¹ä¸»ç«™ï¼Œéƒ¨åˆ†VACEçš„ä¾¿æ·åŠŸèƒ½ä¸åœ¨çº¿ä½“éªŒå…¥å£ä¹Ÿå³å°†ä¸Šçº¿ï¼Œä¸ºæ‚¨æä¾›æ›´å¤šæ”¯æŒã€‚", "en": "Stay Updated with Official Channels: Keep a close watch on the official Tongyi Wanxiang main website, as some user-friendly VACE features and online experience portals will soon be launched, providing you with more support." },
            "relevantLinks": { "zh": "ç›¸å…³æ ¸å¿ƒèµ„æºé“¾æ¥ç›´è¾¾ï¼š", "en": "Direct Links to Core Relevant Resources:" },
            "officialExperience": { "zh": "é€šä¹‰ä¸‡ç›¸å®˜æ–¹åœ¨çº¿ä½“éªŒä¸æ¢ç´¢å…¥å£ï¼š", "en": "Official Tongyi Wanxiang Online Experience & Exploration Portals:" },
            "domesticPortal": { "zh": "å›½å†…ç«™ (é˜¿é‡Œäº‘é€šä¹‰)", "en": "Domestic Site (Alibaba Cloud Tongyi)" },
            "internationalPortal": { "zh": "å›½é™…ç«™ (Wan.Video Platform)", "en": "International Site (Wan.Video Platform)" },

            // Footer
            "footerCopyright": { "zh": "é€šä¹‰å¤§æ¨¡å‹å›¢é˜Ÿ. ç‰ˆæƒæ‰€æœ‰.", "en": "Tongyi Large Model Team. All Rights Reserved." },
            "footerDisclaimer": { "zh": "æœ¬æ–‡æ¡£æ—¨åœ¨æä¾›å…³äºé€šä¹‰ä¸‡ç›¸VACEæ¨¡å‹çš„å¼€æºä¿¡æ¯ã€‚å…·ä½“æ¨¡å‹ä½¿ç”¨è¯·ä¸¥æ ¼éµå¾ªå®˜æ–¹å‘å¸ƒçš„è®¸å¯åè®®å’Œä½¿ç”¨æŒ‡å—ï¼Œç¡®ä¿åˆè§„ã€‚", "en": "This document aims to provide open-source information about the Tongyi Wanxiang VACE model. For specific model usage, please strictly adhere to the officially released licensing agreements and usage guidelines to ensure compliance." },
            "footerICP": { "zh": "æ¸ICPå¤‡2025053282å·", "en": "Yu ICP Bei 2025053282" }
        };

        let currentLanguage = 'en'; // Default language
        const languageToggleBtn = document.getElementById('languageToggleBtn');
        const htmlEl = document.documentElement;

        function applyTranslations() {
            htmlEl.lang = translations.htmlLang[currentLanguage];
            if (currentLanguage === 'zh') {
                document.body.setAttribute('lang', 'zh-CN');
            } else {
                document.body.removeAttribute('lang');
            }

            document.querySelectorAll('[data-lang-key]').forEach(element => {
                const key = element.getAttribute('data-lang-key');
                if (translations[key] && translations[key][currentLanguage]) {
                    element.innerHTML = translations[key][currentLanguage];
                }
            });
            languageToggleBtn.textContent = currentLanguage === 'zh' ? 'English' : 'ä¸­æ–‡';
            activateNavLink();
        }

        languageToggleBtn.addEventListener('click', () => {
            currentLanguage = currentLanguage === 'zh' ? 'en' : 'zh';
            applyTranslations();
        });

        document.addEventListener('DOMContentLoaded', () => {
            applyTranslations();
            activateNavLink();
        });
    </script>

</body>
</html>